# T·ªëi ∆∞u Hi·ªáu su·∫•t & Ki·ªÉm th·ª≠ Database ‚Äì ƒê·ªÅ c∆∞∆°ng Bu·ªïi ƒê√†o t·∫°o (60 ph√∫t)

---
### **Gi·∫£i th√≠ch ph·∫ßn ti√™u ƒë·ªÅ**
*Bu·ªïi h·ªçc n√†y chuy·ªÉn t·ª´ thi·∫øt k·∫ø l√Ω thuy·∫øt sang **production-ready performance**. H·ªçc vi√™n s·∫Ω h·ªçc c√°ch ƒë√°nh gi√°, t·ªëi ∆∞u v√† ki·ªÉm th·ª≠ database ƒë·ªÉ ƒë·∫£m b·∫£o h·ªá th·ªëng c√≥ th·ªÉ handle traffic th·ª±c t·∫ø. ƒê√¢y l√† k·ªπ nƒÉng then ch·ªët ƒë·ªÉ database design kh√¥ng ch·ªâ ƒë·∫πp tr√™n gi·∫•y m√† c√≤n ho·∫°t ƒë·ªông m∆∞·ª£t m√† trong th·ª±c t·∫ø.*

**ƒê·ªëi t∆∞·ª£ng h·ªçc vi√™n:**
Full-stack developers ƒë√£ n·∫Øm v·ªØng ERD design, mu·ªën h·ªçc c√°ch **optimize database performance** cho production environment. ƒê·∫∑c bi·ªát ph√π h·ª£p v·ªõi nh·ªØng ng∆∞·ªùi chu·∫©n b·ªã launch s·∫£n ph·∫©m ho·∫∑c ƒëang g·∫∑p performance issues trong d·ª± √°n hi·ªán t·∫°i.

**M·ª•c ti√™u bu·ªïi h·ªçc:**
- **ƒê√°nh gi√° performance** database th√¥ng qua c√°c metrics quan tr·ªçng
- **Th√†nh th·∫°o indexing strategy** ƒë·ªÉ t·ªëi ∆∞u query speed
- **√Åp d·ª•ng load testing** v·ªõi c√¥ng c·ª• chuy√™n d·ª•ng 
- **C√¢n b·∫±ng trade-offs** gi·ªØa normalization v√† performance
- **Troubleshoot** c√°c v·∫•n ƒë·ªÅ performance ph·ªï bi·∫øn trong production

---

## 1. D·∫´n nh·∫≠p (15 ph√∫t)

### **1.1. Kh·ªüi ƒë·ªông & C√¢u h·ªèi m·ªü**
*Ai ƒë√£ t·ª´ng g·∫∑p nh·ªØng t√¨nh hu·ªëng n√†y trong production:*
- **"Website ch·∫°y ngon ·ªü local nh∆∞ng ch·∫≠m nh∆∞ r√πa khi c√≥ user th·∫≠t?"** *(Pause ƒë·ªÉ nghe ph·∫£n h·ªìi)*
- **"Query ƒë∆°n gi·∫£n nh∆∞ng m·∫•t 5-10 gi√¢y ƒë·ªÉ tr·∫£ k·∫øt qu·∫£?"**
- **"Database CPU spike l√™n 100% m·ªói khi c√≥ traffic cao?"**

*ƒê√¢y ch√≠nh l√† nh·ªØng d·∫•u hi·ªáu c·ªßa **database performance bottlenecks**. H√¥m nay ch√∫ng ta s·∫Ω h·ªçc c√°ch identify, measure v√† fix nh·ªØng v·∫•n ƒë·ªÅ n√†y m·ªôt c√°ch systematic.*

### **1.2. C√¢u chuy·ªán d·∫´n d·∫Øt**
**Stack Overflow's 25-Second Query Crisis (2008):**

*NƒÉm 2008, Stack Overflow - website Q&A l·ªõn nh·∫•t cho developers - g·∫∑p ph·∫£i m·ªôt cu·ªôc kh·ªßng ho·∫£ng performance nghi√™m tr·ªçng. **M·ªôt query ƒë∆°n gi·∫£n ƒë·ªÉ load homepage m·∫•t t·ªõi 25 gi√¢y!***

*V·∫•n ƒë·ªÅ? Database c√≥ 50 tri·ªáu records nh∆∞ng **kh√¥ng c√≥ index n√†o** tr√™n c√°c c·ªôt th∆∞·ªùng xuy√™n query. Jeff Atwood (co-founder) k·ªÉ l·∫°i:*

> *"Ch√∫ng t√¥i ƒë√£ thi·∫øt k·∫ø schema r·∫•t ƒë·∫πp, normalized ho√†n h·∫£o, nh∆∞ng qu√™n m·∫•t performance. M·ªôt query JOIN 4 b·∫£ng ƒë·ªÉ hi·ªÉn th·ªã danh s√°ch c√¢u h·ªèi ph·∫£i scan qua 50 tri·ªáu rows m·ªói l·∫ßn."*

**Gi·∫£i ph√°p c·ªßa h·ªç:**
1. **Strategic indexing:** Th√™m composite indexes cho common query patterns
2. **Denormalization c√≥ ki·ªÉm so√°t:** Pre-calculate view counts, vote scores
3. **Query optimization:** Rewrite expensive JOINs th√†nh efficient lookups

**K·∫øt qu·∫£:** *Query time gi·∫£m t·ª´ 25 gi√¢y xu·ªëng **50 milliseconds** - c·∫£i thi·ªán 500 l·∫ßn!*

*Stack Overflow hi·ªán handle **6,000 requests/second** v·ªõi ch·ªâ 9 database servers. **Performance optimization t·ªët = infrastructure cost th·∫•p = business success.***

### **1.3. V√≠ d·ª• h·∫•p d·∫´n**
**Discord's Database Scaling Magic:**

*Discord ph·ª•c v·ª• **150 tri·ªáu active users** v·ªõi database architecture c·ª±c k·ª≥ th√¥ng minh. B√≠ m·∫≠t c·ªßa h·ªç kh√¥ng ph·∫£i l√† hardware ƒë·∫Øt ti·ªÅn, m√† l√† **performance optimization strategies**:*

**Message Storage Optimization:**
```sql
-- Thay v√¨ store t·∫•t c·∫£ messages trong 1 b·∫£ng kh·ªïng l·ªì
CREATE TABLE messages_2024_01 (...);  -- Partition by month
CREATE TABLE messages_2024_02 (...);  

-- V·ªõi strategic indexes:
CREATE INDEX idx_messages_channel_timestamp 
ON messages_2024_01 (channel_id, created_at DESC);
```

**K·∫øt qu·∫£ ·∫•n t∆∞·ª£ng:**
- **Truy v·∫•n message history:** < 10ms response time
- **Database size:** 1+ petabyte data
- **Query throughput:** 40,000+ queries/second
- **Infrastructure cost:** 90% th·∫•p h∆°n so v·ªõi naive approach

*Discord ch·ª©ng minh: **Smart performance optimization > Expensive hardware.***

### **1.4. Gi·ªõi thi·ªáu ch·ªß ƒë·ªÅ**
*"H√¥m nay ch√∫ng ta s·∫Ω h·ªçc **Performance Engineering** cho database - t·ª´ vi·ªác ƒëo l∆∞·ªùng metrics quan tr·ªçng, thi·∫øt k·∫ø indexing strategy hi·ªáu qu·∫£, ƒë·∫øn load testing ƒë·ªÉ ƒë·∫£m b·∫£o h·ªá th·ªëng ready cho production traffic. Ch√∫ng ta s·∫Ω practice v·ªõi tools th·ª±c t·∫ø v√† case studies t·ª´ nh·ªØng startup ƒë√£ scale th√†nh c√¥ng."*

---

## 2. Nguy√™n t·∫Øc & Quy tr√¨nh c·ªët l√µi (20 ph√∫t)

### **2.1. Performance Metrics Framework**

#### **Core Metrics cho Small/Medium Projects:**

| Metric | √ù nghƒ©a | Target cho SME | Measurement Tool |
|--------|---------|----------------|------------------|
| **Response Time** | Th·ªùi gian query execution | < 100ms (p95) | EXPLAIN ANALYZE |
| **Throughput** | Queries per second | > 1,000 QPS | pgbench, Artillery |
| **Resource Usage** | CPU, Memory, I/O | < 70% sustained | pg_stat_activity |
| **Connection Pool** | Active connections | < 80% pool size | Connection monitoring |
| **Cache Hit Ratio** | Buffer cache efficiency | > 95% | pg_stat_database |

#### **Performance Measurement Process:**

```mermaid
graph TD
    A["üìä BASELINE<br/>Establish current performance"] --> B["üéØ TARGET<br/>Define performance goals"] 
    B --> C["üîç IDENTIFY<br/>Find bottlenecks"]
    C --> D["‚ö° OPTIMIZE<br/>Apply improvements"]
    D --> E["‚úÖ VALIDATE<br/>Measure improvements"]
    E --> A
    
    style A fill:#e3f2fd
    style B fill:#f3e5f5  
    style C fill:#ffeb3b
    style D fill:#4caf50
    style E fill:#ff9800
```

### **2.2. Indexing Strategy cho Small/Medium Projects**

#### **Index Types v√† Use Cases:**

**1. B-Tree Index (Default - 90% cases):**
```sql
-- Single column index cho WHERE clauses
CREATE INDEX idx_orders_customer_id ON orders (customer_id);

-- Composite index cho multiple conditions
CREATE INDEX idx_orders_status_date ON orders (status, order_date DESC);

-- Covering index ƒë·ªÉ avoid table lookups
CREATE INDEX idx_orders_summary ON orders (customer_id, status) 
INCLUDE (total_amount, order_date);
```

**2. Partial Index (Filtered data):**
```sql
-- Ch·ªâ index active records (ti·∫øt ki·ªám space)
CREATE INDEX idx_active_orders ON orders (customer_id, order_date) 
WHERE status IN ('pending', 'processing');

-- Index cho rare conditions
CREATE INDEX idx_failed_payments ON payments (order_id, created_at)
WHERE status = 'failed';
```

**3. Expression Index (Computed values):**
```sql
-- Index cho case-insensitive search
CREATE INDEX idx_customers_email_lower ON customers (LOWER(email));

-- Index cho date calculations
CREATE INDEX idx_orders_month ON orders (EXTRACT(MONTH FROM order_date));
```

#### **Indexing Best Practices cho SME:**

**‚úÖ DO - Strategic Indexing:**
```sql
-- 1. Index foreign keys (lu√¥n lu√¥n)
CREATE INDEX idx_orders_customer_id ON orders (customer_id);
CREATE INDEX idx_order_items_order_id ON order_items (order_id);

-- 2. Index search columns
CREATE INDEX idx_products_name_gin ON products USING GIN (to_tsvector('english', name));

-- 3. Index sort columns
CREATE INDEX idx_orders_date_desc ON orders (order_date DESC);

-- 4. Composite index theo query patterns
-- Query: WHERE customer_id = ? AND status = ? ORDER BY order_date DESC
CREATE INDEX idx_orders_customer_status_date ON orders (customer_id, status, order_date DESC);
```

**‚ùå DON'T - Over-indexing:**
```sql
-- Tr√°nh index m·ªçi column
CREATE INDEX idx_orders_notes ON orders (notes);  -- Rarely queried
CREATE INDEX idx_customers_created_at ON customers (created_at);  -- No business value

-- Tr√°nh duplicate indexes
CREATE INDEX idx_orders_customer ON orders (customer_id);
CREATE INDEX idx_orders_customer_duplicate ON orders (customer_id, id);  -- Redundant
```

### **2.3. Query Optimization Techniques**

#### **EXPLAIN ANALYZE - Reading Query Plans:**

```sql
-- Analyze slow query
EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) 
SELECT c.name, COUNT(o.id) as order_count, SUM(o.total_amount) as total_spent
FROM customers c 
LEFT JOIN orders o ON c.id = o.customer_id 
WHERE c.created_at >= '2024-01-01'
GROUP BY c.id, c.name 
ORDER BY total_spent DESC 
LIMIT 10;
```

**Key Indicators trong Query Plan:**
- **Seq Scan:** üö® Needs index
- **Index Scan:** ‚úÖ Good
- **Nested Loop:** üö® Check join conditions  
- **Hash Join:** ‚úÖ Efficient for large datasets
- **Sort:** üö® Consider index for ORDER BY

#### **Common Query Optimization Patterns:**

**1. N+1 Query Problem:**
```sql
-- ‚ùå BAD: N+1 queries
-- First query: SELECT * FROM orders WHERE customer_id = ?
-- Then N queries: SELECT * FROM customers WHERE id = ? (for each order)

-- ‚úÖ GOOD: Single JOIN query
SELECT o.*, c.name, c.email
FROM orders o
JOIN customers c ON o.customer_id = c.id
WHERE o.order_date >= '2024-01-01';
```

**2. Pagination Optimization:**
```sql
-- ‚ùå BAD: OFFSET gets slower as page number increases
SELECT * FROM orders ORDER BY id LIMIT 20 OFFSET 10000;

-- ‚úÖ GOOD: Cursor-based pagination
SELECT * FROM orders 
WHERE id > 10020  -- Last ID from previous page
ORDER BY id LIMIT 20;
```

**3. Aggregation Optimization:**
```sql
-- ‚ùå BAD: Expensive GROUP BY on large table
SELECT customer_id, COUNT(*), SUM(total_amount)
FROM orders 
GROUP BY customer_id;

-- ‚úÖ GOOD: Materialized view or summary table
CREATE MATERIALIZED VIEW customer_stats AS
SELECT customer_id, COUNT(*) as order_count, SUM(total_amount) as total_spent
FROM orders 
GROUP BY customer_id;

-- Refresh periodically
REFRESH MATERIALIZED VIEW customer_stats;
```

### **2.4. Denormalization Strategy cho Performance**

#### **When to Denormalize:**
- **Read-heavy workloads** (10:1 read/write ratio)
- **Expensive JOINs** affecting user experience  
- **Real-time requirements** (< 50ms response time)
- **Reporting queries** v·ªõi complex aggregations

#### **Controlled Denormalization Examples:**

**1. Customer Name trong Orders:**
```sql
-- Thay v√¨ JOIN ƒë·ªÉ l·∫•y customer name m·ªói l·∫ßn
ALTER TABLE orders ADD COLUMN customer_name VARCHAR(100);

-- Update trigger ƒë·ªÉ maintain consistency
CREATE OR REPLACE FUNCTION update_customer_name_in_orders()
RETURNS TRIGGER AS $$
BEGIN
    UPDATE orders SET customer_name = NEW.name WHERE customer_id = NEW.id;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trigger_update_customer_name
    AFTER UPDATE OF name ON customers
    FOR EACH ROW EXECUTE FUNCTION update_customer_name_in_orders();
```

**2. Aggregated Data:**
```sql
-- Store calculated values ƒë·ªÉ avoid expensive aggregations
ALTER TABLE restaurants ADD COLUMN avg_rating DECIMAL(2,1) DEFAULT 0;
ALTER TABLE restaurants ADD COLUMN total_orders INT DEFAULT 0;

-- Background job update aggregates
UPDATE restaurants SET 
    avg_rating = (SELECT AVG(rating) FROM reviews WHERE restaurant_id = restaurants.id),
    total_orders = (SELECT COUNT(*) FROM orders WHERE restaurant_id = restaurants.id);
```

**3. JSONB cho Flexible Data:**
```sql
-- Thay v√¨ multiple JOINs cho order details
ALTER TABLE orders ADD COLUMN order_summary JSONB;

-- Example data:
-- {
--   "items": [{"name": "Pizza", "quantity": 2, "price": 25.99}],
--   "restaurant": {"name": "Joe's Pizza", "cuisine": "Italian"},
--   "delivery": {"address": "123 Main St", "estimated_time": "30 mins"}
-- }

-- Query with JSONB
SELECT * FROM orders 
WHERE order_summary->>'restaurant'->>'cuisine' = 'Italian';
```

---

## 3. √Åp d·ª•ng th·ª±c t·∫ø & Case studies (15 ph√∫t)

### **3.1. Case Study: E-commerce Performance Crisis**

**Scenario:** *E-commerce startup v·ªõi 10K users, 100K products, 500K orders. Homepage load time tƒÉng t·ª´ 200ms l√™n 5 gi√¢y khi traffic tƒÉng 3x.*

#### **Step 1: Performance Diagnosis**

**Slow Query Identification:**
```sql
-- Enable slow query logging
ALTER SYSTEM SET log_min_duration_statement = 1000; -- Log queries > 1s
SELECT pg_reload_conf();

-- Check slow queries
SELECT query, mean_exec_time, calls, total_exec_time
FROM pg_stat_statements 
ORDER BY mean_exec_time DESC 
LIMIT 10;
```

**Found Issues:**
1. **Product search:** 8.5s average (no text search index)
2. **Order history:** 3.2s average (missing composite index)
3. **Category listing:** 2.1s average (N+1 query problem)

#### **Step 2: Optimization Implementation**

**1. Text Search Optimization:**
```sql
-- Add GIN index cho full-text search
ALTER TABLE products ADD COLUMN search_vector tsvector;

UPDATE products SET search_vector = 
    to_tsvector('english', name || ' ' || COALESCE(description, ''));

CREATE INDEX idx_products_search ON products USING GIN (search_vector);

-- Optimized search query
SELECT * FROM products 
WHERE search_vector @@ plainto_tsquery('english', 'wireless headphones')
ORDER BY ts_rank(search_vector, plainto_tsquery('english', 'wireless headphones')) DESC;
```

**2. Order History Optimization:**
```sql
-- Composite index cho common query pattern
CREATE INDEX idx_orders_customer_date_status ON orders (customer_id, order_date DESC, status);

-- Covering index ƒë·ªÉ avoid table lookup
CREATE INDEX idx_orders_summary ON orders (customer_id, order_date DESC) 
INCLUDE (total_amount, status, shipping_address);
```

**3. Category N+1 Fix:**
```sql
-- ‚ùå Before: N+1 queries
-- SELECT * FROM categories;
-- For each category: SELECT COUNT(*) FROM products WHERE category_id = ?

-- ‚úÖ After: Single query with LEFT JOIN
SELECT c.*, COUNT(p.id) as product_count
FROM categories c
LEFT JOIN products p ON c.id = p.category_id
GROUP BY c.id, c.name, c.description
ORDER BY c.sort_order;
```

#### **Step 3: Results & Metrics**

| Metric | Before | After | Improvement |
|--------|---------|-------|-------------|
| **Homepage Load** | 5.2s | 180ms | **96% faster** |
| **Search Response** | 8.5s | 45ms | **99% faster** |
| **Order History** | 3.2s | 120ms | **96% faster** |
| **Database CPU** | 85% avg | 35% avg | **59% reduction** |
| **Concurrent Users** | 100 max | 500+ stable | **5x capacity** |

### **3.2. Load Testing Strategy**

#### **Progressive Load Testing Approach:**

```mermaid
graph TD
    A["üîß Unit Tests<br/>Individual queries"] --> B["üèÉ Smoke Tests<br/>Basic functionality"] 
    B --> C["üìà Load Tests<br/>Expected traffic"]
    C --> D["üí• Stress Tests<br/>Breaking point"]
    D --> E["‚è∞ Endurance Tests<br/>Sustained load"]
    
    style A fill:#e8f5e8
    style B fill:#fff3e0  
    style C fill:#e3f2fd
    style D fill:#ffebee
    style E fill:#f3e5f5
```

#### **Tool Setup: Artillery.js cho Database Testing**

**1. Installation & Basic Config:**
```bash
npm install -g artillery
```

**2. Database Load Test Configuration:**
```yaml
# artillery-db-test.yml
config:
  target: 'http://localhost:3000'
  phases:
    # Warm-up phase
    - duration: 60
      arrivalRate: 5
      name: "Warm up"
    # Normal load
    - duration: 300
      arrivalRate: 20
      name: "Normal load"
    # Peak load
    - duration: 120
      arrivalRate: 50
      name: "Peak load"
    # Stress test
    - duration: 60
      arrivalRate: 100
      name: "Stress test"

scenarios:
  - name: "E-commerce user journey"
    weight: 70
    flow:
      - get:
          url: "/api/products?search=laptop"
      - think: 2
      - get:
          url: "/api/products/{{ $randomInt(1, 1000) }}"
      - think: 3
      - post:
          url: "/api/cart/add"
          json:
            product_id: "{{ $randomInt(1, 1000) }}"
            quantity: "{{ $randomInt(1, 3) }}"
      
  - name: "Admin operations"
    weight: 30
    flow:
      - get:
          url: "/api/admin/orders?page={{ $randomInt(1, 10) }}"
      - get:
          url: "/api/admin/analytics/revenue"
```

**3. Run Tests:**
```bash
# Basic load test
artillery run artillery-db-test.yml

# With detailed reporting
artillery run --output report.json artillery-db-test.yml
artillery report report.json
```

#### **pgbench for Database-Specific Testing:**

```bash
# Initialize test database
pgbench -i -s 50 testdb  # Scale factor 50 = ~5M rows

# Custom test script
cat > custom-test.sql << EOF
\set customer_id random(1, 100000)
\set product_id random(1, 10000)
SELECT c.name, COUNT(o.id) as orders 
FROM customers c 
LEFT JOIN orders o ON c.id = o.customer_id 
WHERE c.id = :customer_id 
GROUP BY c.id, c.name;
EOF

# Run performance test
pgbench -c 20 -j 4 -T 300 -f custom-test.sql testdb
```

### **3.3. Production Monitoring Setup**

#### **Key Metrics Dashboard:**

**1. Query Performance Monitoring:**
```sql
-- Create monitoring view
CREATE VIEW slow_queries AS
SELECT 
    query,
    calls,
    total_exec_time,
    mean_exec_time,
    rows,
    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
FROM pg_stat_statements 
WHERE mean_exec_time > 100  -- Queries slower than 100ms
ORDER BY mean_exec_time DESC;
```

**2. Connection Monitoring:**
```sql
-- Monitor active connections
SELECT 
    datname,
    usename,
    application_name,
    client_addr,
    state,
    query_start,
    state_change,
    query
FROM pg_stat_activity 
WHERE state = 'active';
```

**3. Index Usage Analysis:**
```sql
-- Find unused indexes
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_tup_read,
    idx_tup_fetch,
    pg_size_pretty(pg_relation_size(indexrelid)) as size
FROM pg_stat_user_indexes 
WHERE idx_tup_read = 0 AND idx_tup_fetch = 0
ORDER BY pg_relation_size(indexrelid) DESC;
```

---

## 4. Th·∫£o lu·∫≠n m·ªü & Workshop mini (7 ph√∫t)

### **Workshop: Performance Troubleshooting Challenge**

**Scenario:** *B·∫°n ƒë∆∞·ª£c g·ªçi ƒë·ªÉ fix performance issue c·ªßa m·ªôt Social Media API. Users complain v·ªÅ slow feed loading v√† timeout errors during peak hours.*

#### **Given Information:**
- **Database:** PostgreSQL v·ªõi 1M users, 10M posts, 100M likes
- **Symptoms:** Feed loading 8-15 seconds, CPU 90%+ during peak
- **Current query:** 
```sql
SELECT p.*, u.username, u.avatar_url, COUNT(l.id) as like_count
FROM posts p
JOIN users u ON p.user_id = u.id  
LEFT JOIN likes l ON p.id = l.post_id
WHERE p.user_id IN (
    SELECT following_id FROM follows WHERE follower_id = ?
)
ORDER BY p.created_at DESC
LIMIT 20;
```

#### **C√¢u h·ªèi th·∫£o lu·∫≠n:**

1. **Performance Diagnosis:**
   *"Nh√¨n v√†o query n√†y, b·∫°n th·∫•y nh·ªØng bottlenecks n√†o? L√†m sao ƒë·ªÉ identify root cause?"*

   **G·ª£i √Ω tr·∫£ l·ªùi:**
   - **Subquery issue:** IN clause v·ªõi subquery kh√¥ng efficient
   - **Missing indexes:** C·∫ßn indexes cho follows table, posts.created_at
   - **Expensive aggregation:** COUNT(likes) cho m·ªói post
   - **N+1 potential:** Avatar loading c√≥ th·ªÉ optimize

2. **Optimization Strategy:**
   *"V·ªõi context social media (read-heavy, real-time), b·∫°n s·∫Ω optimize nh∆∞ th·∫ø n√†o? Trade-offs g√¨ c·∫ßn consider?"*

   **G·ª£i √Ω tr·∫£ l·ªùi:**
   - **Denormalize like_count** trong posts table (update via trigger)
   - **Rewrite subquery** th√†nh JOIN v·ªõi follows table
   - **Add strategic indexes:** composite index cho (follower_id, following_id)
   - **Consider caching:** Redis cho popular feeds

3. **Load Testing Plan:**
   *"L√†m sao test performance improvements? Metrics n√†o quan tr·ªçng nh·∫•t cho social media?"*

   **G·ª£i √Ω tr·∫£ l·ªùi:**
   - **Key metrics:** Feed load time < 500ms, 1000+ concurrent users
   - **Test scenarios:** Peak posting hours, viral content spikes
   - **Tools:** Artillery cho API testing, pgbench cho database load

**Interactive Exercise (3 ph√∫t):**
*M·ªçi ng∆∞·ªùi h√£y vi·∫øt optimized version c·ªßa query tr√™n. Ai mu·ªën share solution v√† explain reasoning?*

**Sample Optimized Solution:**
```sql
-- Optimized query v·ªõi denormalized data
SELECT 
    p.id, p.content, p.created_at, p.like_count,  -- Pre-calculated
    u.username, u.avatar_url
FROM posts p
JOIN follows f ON p.user_id = f.following_id
JOIN users u ON p.user_id = u.id
WHERE f.follower_id = ?
ORDER BY p.created_at DESC
LIMIT 20;

-- Required indexes:
-- CREATE INDEX idx_follows_follower_following ON follows (follower_id, following_id);
-- CREATE INDEX idx_posts_user_created ON posts (user_id, created_at DESC);
```

---

## 5. T√≥m t·∫Øt (3 ph√∫t)

### **5.1. T·ªïng k·∫øt c√°c nguy√™n t·∫Øc ch√≠nh**
- **Performance Framework:** Baseline ‚Üí Target ‚Üí Identify ‚Üí Optimize ‚Üí Validate
- **Strategic Indexing:** Foreign keys, search columns, sort columns, composite indexes
- **Query Optimization:** Avoid N+1, efficient JOINs, cursor pagination
- **Controlled Denormalization:** Read-heavy workloads, real-time requirements

### **5.2. Nh·∫•n m·∫°nh mindset c·ªët l√µi**
**"Performance optimization is not about premature optimization - it's about measuring first, optimizing strategically, and validating results."**

### **5.3. C·∫ßu n·ªëi ƒë·∫øn b√†i h·ªçc ti·∫øp theo**
*"H√¥m nay ch√∫ng ta ƒë√£ h·ªçc optimize database performance - the engine. Bu·ªïi cu·ªëi s·∫Ω h·ªçc tri·ªÉn khai v·ªõi Prisma ORM v√† NestJS - bringing everything together into production-ready application. T·ª´ optimized database ƒë·∫øn modern API!"*

---

## 6. T√†i li·ªáu tham kh·∫£o & Next steps

### **Tools v√† Resources:**
- **Artillery.js:** Load testing cho APIs v√† databases
- **pgbench:** PostgreSQL performance benchmarking  
- **pg_stat_statements:** Query performance analysis
- **EXPLAIN ANALYZE:** Query execution plan analysis

### **Practice Exercises:**
- **Exercise 1:** Optimize slow queries trong existing project
- **Exercise 2:** Set up comprehensive load testing suite
- **Exercise 3:** Create performance monitoring dashboard

### **Advanced Reading:**
- "PostgreSQL Performance Tuning" - Greg Smith
- "High Performance PostgreSQL" - Gregory Smith
- Artillery.js Documentation: Database Load Testing
- PostgreSQL Documentation: Performance Tips 